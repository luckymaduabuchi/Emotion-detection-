{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4110254b-6069-4e33-8e18-27892076b4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucky/anaconda3/envs/diarization/lib/python3.9/site-packages/s3prl/upstream/byol_s/byol_a/common.py:20: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"sox_io\")\n",
      "ESPnet is not installed, cannot use espnet_hubert upstream\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from s3prl.hub import distilhubert\n",
    "from s3prl.optimizers import WarmupLinearSchedule\n",
    "import time\n",
    "\n",
    "# Allowlist required classes for torch.load\n",
    "torch.serialization.add_safe_globals([WarmupLinearSchedule, argparse.Namespace])\n",
    "\n",
    "# Set random seed\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8883e130-643a-465f-9c37-bf9b1eff45aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UpstreamExpert] - Using the default upstream expert config\n",
      "[DistillerModel] - Expands the output dimension by 3 times\n",
      "[DistillerModel] - Pred layers: [4, 8, 12]\n",
      "[TransformerEncoder] - Attention type = original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucky/anaconda3/envs/diarization/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DistillerModel] - Out layer type: expand-last\n",
      "[DistillerModel] - Inter dim = 768\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load pretrained model\n",
    "pretrained_model = distilhubert().to(device)\n",
    "\n",
    "# Emotion class mapping\n",
    "emotion_classes = {\n",
    "    0: \"happy\",\n",
    "    1: \"sad\",\n",
    "    2: \"angry\",\n",
    "    3: \"fear\",\n",
    "    4: \"disgust\",\n",
    "    5: \"neutral\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7202d276-29fd-4a76-98c4-1bf9c892e62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, audio_dir, labels_file, max_duration=10.0, augment=False):\n",
    "        self.audio_files = []\n",
    "        self.labels = []\n",
    "        self.max_samples = int(16000 * max_duration)\n",
    "        self.augment = augment\n",
    "\n",
    "        with open(labels_file, 'r') as file:\n",
    "            next(file)\n",
    "            for line in file:\n",
    "                columns = line.strip().split(',')\n",
    "                audio_file = columns[0]\n",
    "                emotion = columns[3]\n",
    "                path = os.path.join(audio_dir, audio_file)\n",
    "                if os.path.exists(path):\n",
    "                    self.audio_files.append(path)\n",
    "                    self.labels.append(self.emotion_to_label(emotion))\n",
    "                else:\n",
    "                    print(f\"Missing file: {path}\")\n",
    "\n",
    "    def emotion_to_label(self, emotion):\n",
    "        return {\"HAP\": 0, \"SAD\": 1, \"ANG\": 2, \"FEA\": 3, \"DIS\": 4, \"NEU\": 5}.get(emotion, -1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def augment_waveform(self, waveform):\n",
    "        # Random gain\n",
    "        if random.random() < 0.5:\n",
    "            gain_db = random.uniform(-6, 6)\n",
    "            waveform = waveform * (10 ** (gain_db / 20))\n",
    "\n",
    "        # Add Gaussian noise\n",
    "        if random.random() < 0.5:\n",
    "            noise = torch.randn_like(waveform) * 0.005\n",
    "            waveform = waveform + noise\n",
    "\n",
    "        # Pitch shift (simulate by resample)\n",
    "        if random.random() < 0.5:\n",
    "            shift = random.randint(-2, 2)\n",
    "            new_freq = int(16000 * (2.0 ** (shift / 12)))\n",
    "            waveform = torchaudio.transforms.Resample(orig_freq=16000, new_freq=new_freq)(waveform)\n",
    "            waveform = torchaudio.transforms.Resample(orig_freq=new_freq, new_freq=16000)(waveform)\n",
    "\n",
    "        # Random time shift\n",
    "        if random.random() < 0.5:\n",
    "            shift = int(16000 * random.uniform(-0.2, 0.2))\n",
    "            waveform = torch.roll(waveform, shifts=shift)\n",
    "\n",
    "        return waveform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        waveform, sample_rate = torchaudio.load(self.audio_files[idx])\n",
    "        if sample_rate != 16000:\n",
    "            waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
    "        waveform = waveform.squeeze(0)[:self.max_samples]\n",
    "\n",
    "        if self.augment:\n",
    "            waveform = self.augment_waveform(waveform)\n",
    "\n",
    "        return waveform, self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f176e50c-c6ed-4b5b-9ff2-7b05d6d98e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Collate Function\n",
    "def collate_fn(batch):\n",
    "    waveforms, labels = zip(*batch)\n",
    "    waveforms = pad_sequence(waveforms, batch_first=True)\n",
    "    labels = torch.tensor(labels)\n",
    "    return waveforms, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17ac0477-6430-41c9-8935-3cfea02e8e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 5209\n",
      "Val set: 1116\n",
      "Test set: 1117\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load Data\n",
    "train_dir = \"/home/lucky/Documents/research/dataset/train\"\n",
    "val_dir = \"/home/lucky/Documents/research/dataset/val\"\n",
    "test_dir = \"/home/lucky/Documents/research/dataset/test\"\n",
    "\n",
    "train_labels_file = \"/home/lucky/Documents/research/dataset/train.csv\"\n",
    "val_labels_file = \"/home/lucky/Documents/research/dataset/val.csv\"\n",
    "test_labels_file = \"/home/lucky/Documents/research/dataset/test.csv\"\n",
    "\n",
    "# ✅ Enable augmentation only for training dataset\n",
    "train_dataset = EmotionDataset(train_dir, train_labels_file, augment=True)\n",
    "val_dataset = EmotionDataset(val_dir, val_labels_file, augment=False)\n",
    "test_dataset = EmotionDataset(test_dir, test_labels_file, augment=False)\n",
    "\n",
    "print(f\"Train set: {len(train_dataset)}\")\n",
    "print(f\"Val set: {len(val_dataset)}\")\n",
    "print(f\"Test set: {len(test_dataset)}\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, collate_fn=collate_fn)  # Reduced batch size to 1 to avoid OOM\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7c98f81-4654-4eec-a891-30e01f6116c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 6: Classifier\n",
    "import torch.nn as nn\n",
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=768, num_classes=6):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "classifier = EmotionClassifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade133b4-27d5-4096-b281-23bbb6e9d2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=1.5239, Accuracy=0.3872, Time=14855.47s\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Training Loop\n",
    "optimizer = torch.optim.AdamW(list(pretrained_model.parameters()) + list(classifier.parameters()), lr=1e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    pretrained_model.train()\n",
    "    classifier.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for waveforms, labels in train_loader:\n",
    "        waveforms, labels = waveforms.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        results = pretrained_model(waveforms)\n",
    "        features = results[\"paper\"]  # shape: (B, T, D)\n",
    "        pooled = features.mean(dim=1)  # shape: (B, D) — average across time\n",
    "        logits = classifier(pooled)\n",
    "\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        torch.cuda.empty_cache()  # Explicitly clear cache to help with fragmentation\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, Accuracy={accuracy:.4f}, Time={time.time() - start_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cb1506-047a-4457-9113-12202eebb104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Evaluation\n",
    "test_preds, test_labels = [], []\n",
    "classifier.eval()\n",
    "pretrained_model.eval()\n",
    "start = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for waveforms, labels in test_loader:\n",
    "        waveforms, labels = waveforms.to(device), labels.to(device)\n",
    "        results = pretrained_model(waveforms)\n",
    "        features = results[\"paper\"]\n",
    "        pooled = features.mean(dim=1)\n",
    "        logits = classifier(pooled)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "end = time.time()\n",
    "accuracy = accuracy_score(test_labels, test_preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, test_preds, average='weighted')\n",
    "inference_time = (end - start) / len(test_loader.dataset)\n",
    "model_size = sum(p.numel() for p in classifier.parameters() if p.requires_grad) * 4 / (1024**2)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "print(f\"Inference time/sample: {inference_time:.4f}s, Model size: {model_size:.2f}MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad0bce7-0441-44f5-8acc-398d23a86c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Confusion Matrix\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=list(emotion_classes.values()), yticklabels=list(emotion_classes.values()))\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# Cell 10: Save Model\n",
    "torch.save(classifier.state_dict(), \"distilhubert_emotion_recognition.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
